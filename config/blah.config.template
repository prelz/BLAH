#Supported batch systems
supported_lrms=pbs,lsf

#DGAS logfile 
BLAHPD_ACCOUNTING_INFO_LOG=

#Set to yes if you wish to disable BLAH's machinery for transferring 
#or delegating proxies to the worker node where a job is running.
blah_disable_wn_proxy_renewal=no

#Set to yes to enable delegation (instead of copy) of renewed proxies
#to worker nodes. NOTE: limited *and* delegated proxes are not 
#accepted for default GSI authentication as of VDT 1.2, so this should
#be enabled only if non-limited proxies are used for proxy renewal.
blah_delegate_renewed_proxies=no

#Path where PBS executables are located 
pbs_binpath=/usr/pbs/bin

#Path where the PBS logs are located ($pbs_spoolpath/server_logs)
pbs_spoolpath=/usr/spool/PBS

#If it is set to yes blah does not check the jobid in the logfiles
pbs_nochecksubmission=

#If it is set to yes blah does NOT use log files to get job status, 
#but uses only standard LRMS query (qstat)  
pbs_nologaccess=

#If it is set to no blah scripts for PBS will not try to read 
#locally from the logs if BLParser is not present
pbs_fallback=no

#Set to yes to use Blah Log Parser for PBS
pbs_BLParser=

#Host where Blah Log Parser for PBS is running
pbs_BLPserver=127.0.0.1

#Port where Blah Log Parser for PBS is running
pbs_BLPport=33332

#Number of Blah Log Parser to try for PBS (if it is null pbs_BLPserver and pbs_BLPport are used)
pbs_num_BLParser=
#
pbs_BLPserver1=
pbs_BLPport1=
#
pbs_BLPserver2=
pbs_BLPport2=

# Certain Torque versions, starting with 2.4.6, don't support the
# multiple stagein/stagout directive documented in the qsub manpage due to a 
# bug.
# As a safe recipe for automatically detecting this condition at runtime cannot
# be established, this is left to manual configuration.
# Setting this attribute to 'yes' -should- be compatible with older
# Torque versions, but may be failing with PBSpro.
blah_torque_multiple_staging_directive_bug=no

####

#Path where LSF executables are located 
lsf_binpath=/usr/local/lsf/bin

#Path where the LSF conf file is located ($lsf_confpath/lsf.conf)
lsf_confpath=/etc

#If it is set to yes blah does not check the jobid in the logfiles
lsf_nochecksubmission=

#If it is set to yes blah does NOT use log files to get job status, 
#but uses only standard LRMS query (bhist)  
lsf_nologaccess=

#If it is set to no blah scripts for LSF will not try to read 
#locally from the logs if BLParser is not present
lsf_fallback=no

#Set to yes to use Blah Log Parser for LSF
lsf_BLParser=

#Host where Blah Log Parser for LSF is running
lsf_BLPserver=127.0.0.1

#Port where Blah Log Parser for LSF is running
lsf_BLPport=33333

#Number of Blah Log Parser to try for LSF (if it is null lsf_BLPserver and lsf_BLPport are used)
lsf_num_BLParser=
#
lsf_BLPserver1=
lsf_BLPport1=
#
lsf_BLPserver2=
lsf_BLPport2=

#
#LSF Updater
#
#number of logs to be read by bhist (default:3)
bhist_logs_to_read=
#
# Condor
#

#Updater location
bupdater_path=

#Notifier location
bnotifier_path=

#Updater pid file
bupdater_pidfile=/var/tmp/cream_tomcat_bupdater.pid

#Notifier pid file
bnotifier_pidfile=/var/tmp/cream_tomcat_bnotifier.pid

#condor bin location
condor_binpath=/opt/condor-c/bin

#Registry file location
job_registry=/var/tmp/cream_tomcat_registry.db
#Set the following variable to 'yes' to have multiple BLAHPD instances
#share the job registry -index- via mmap:
job_registry_use_mmap=no

#host for asyncronous notification 
async_notification_host=

#port for asyncronous notification
async_notification_port=

#bupdater debug level
bupdater_debug_level=1

#bupdater debug log file
bupdater_debug_logfile=/var/tmp/bupdater.log

#bnotifier debug level
bnotifier_debug_level=1

#bnotifier debug log file
bnotifier_debug_logfile=/var/tmp/bnotifier.log

# purge interval
purge_interval=7200

#after that interval a bhist with -n bhist_logs_to_read is tried (default:120)
bhist_finalstate_interval=120

#Minimum interval of time between the last update of a jobid entry and the first finalstate query try (default:30)
finalstate_query_interval=30

#after that interval an unseen job is set as done (status == 4) and exitstatus == 999 (default:3600)
alldone_interval=3600

#path to condor_config
export CONDOR_CONFIG="/opt/condor-c/etc/condor_config"

#max number of concurrent threads to serve commands (default = 500)
#blah_max_threaded_cmds=100

#seconds to sleep in the main loop
#loop_interval=

#use the long format for bjobs command (-l instead of -w) (yes|no) (default=yes)
bupdater_bjobs_long_format=yes

#use bhist to calculate suspended jobs timestamp 
bupdater_use_bhist_for_susp=no

#Colon-separated list of paths that are shared among batch system
#head and worker nodes.
#blah_shared_directories=/home:/users

#By default the job temporary work directory is created as a subdirectory
#of wherever the batch system is configured to land the job. 
#This variable changes the location where the work directory is created.
#A shell variable escaped or in single quotes will be resolved on the 
#worker node in the job environment. Non-escaped variables will be resolved 
#on the submit node in the submit environment.
#blah_wn_temporary_home_dir='$GLITE_LOCATION_TMP'

#These two attributes allow to change the directory on the worker node where 
#the batch system is instructed to transfer input/output sandbox files to 
#and from.
#These can be set in case the batch system default is not good enough
#(e.g.: the batch systems leaves output files behind)
#These variables can be resolved on the submit node -only-.
#blah_wn_inputsandbox=/tmp
#blah_wn_outputsandbox=/tmp

